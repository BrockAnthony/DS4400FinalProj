{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a5bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dde525",
   "metadata": {},
   "source": [
    "## What Question is Being Answered?\n",
    "\n",
    "The problem aims to predict if an account holder will fail to make a payment next month based on their payment history and other banking details. This is a binary classification task since the outcomes are either 'Default' or 'Not Default'. Thus, the target variable is categorical with two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27ec94",
   "metadata": {},
   "source": [
    "## Dataset EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'default of credit card clients.xls'\n",
    "\n",
    "df = pd.read_excel(file)\n",
    "\n",
    "# Drop the first column from the DataFrame\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Replace the column names with the values from the second row\n",
    "new_column_names = df.iloc[0]\n",
    "df.columns = new_column_names\n",
    "\n",
    "# Resetting the index\n",
    "df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "df = df.rename(columns={\"default payment next month\": \"Y\"})\n",
    "\n",
    "# Display the DataFrame with the updated column names\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028e62c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f740df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Payment Delay\n",
    "df['avg_payment_delay'] = df[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].mean(axis=1)\n",
    "\n",
    "# Outstanding Balance for each month\n",
    "for i in range(1, 7):\n",
    "    df[f'outstanding_balance_{i}'] = df[f'BILL_AMT{i}'] - df[f'PAY_AMT{i}']\n",
    "\n",
    "# Credit Utilization for each month\n",
    "for i in range(1, 7):\n",
    "    df[f'credit_utilization_{i}'] = df[f'BILL_AMT{i}'] / df['LIMIT_BAL']\n",
    "\n",
    "# Replacing any infinity values with NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Debt Ratio for each month\n",
    "for i in range(1, 7):\n",
    "    # Avoid division by zero\n",
    "    df[f'debt_ratio_{i}'] = (df[f'BILL_AMT{i}'] - df[f'PAY_AMT{i}']) / df[f'BILL_AMT{i}']\n",
    "    df.loc[df[f'BILL_AMT{i}'] == 0, f'debt_ratio_{i}'] = 0\n",
    "\n",
    "# Age Binning\n",
    "age_bins = [20, 30, 40, 50, 60, 70, 80]\n",
    "age_labels = ['20-30', '30-40', '40-50', '50-60', '60-70', '70-80']\n",
    "df['age_group'] = pd.cut(df['AGE'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "# Consistent Bill Amount\n",
    "df['consistent_bill_amount'] = df[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].std(axis=1)\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\n",
    "                   'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "                   'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "\n",
    "# Drop the specified columns\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f373227",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893036a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of each class in 'Y' column\n",
    "y_counts = df['Y'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,6))  # Set the figure size\n",
    "sns.barplot(x=y_counts.index, y=y_counts.values, palette=\"muted\")  # Create a barplot\n",
    "\n",
    "plt.title('Default Payment Next Month (Y)', fontsize=14)  # Set the title and its fontsize\n",
    "plt.xlabel('Default Payment', fontsize=12)  # Set x-axis label and its fontsize\n",
    "plt.ylabel('Count', fontsize=12)  # Set y-axis label and its fontsize\n",
    "\n",
    "# Annotate the heights of bars for better visual understanding\n",
    "for i, v in enumerate(y_counts.values):\n",
    "    plt.text(i, v, str(v), ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.xticks(ticks=[0, 1], labels=['(0) No', '(1) Yes'])  # Replace 0,1 labels with No and Yes for better understanding\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27bd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix for all features\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Set up the figure size\n",
    "plt.figure(figsize=(18, 15))\n",
    "\n",
    "# Create a heatmap with reduced font size in annotations and set a threshold for displaying annotations\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", annot_kws={'size': 8})\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title('Correlation Heatmap of All Features', fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix for all features\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Set a threshold for significant correlations\n",
    "threshold = 0.5\n",
    "\n",
    "# Mask the values that are below the threshold\n",
    "mask = np.abs(correlation_matrix) < threshold\n",
    "\n",
    "plt.figure(figsize=(18, 15))\n",
    "\n",
    "# Create a heatmap with masked insignificant correlations, and reduced font size in annotations\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap=\"coolwarm\", fmt=\".2f\", annot_kws={'size': 8})\n",
    "\n",
    "plt.title('Correlation Heatmap of Significant Features (> 0.5)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00a1db",
   "metadata": {},
   "source": [
    "## Model Selection and Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f606b25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=10000),\n",
    "    'Support Vector Machines': LinearSVC(max_iter=10000, C=10),\n",
    "    'Decision Trees': DecisionTreeClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'EasyEnsemble': EasyEnsembleClassifier(n_estimators = 200)\n",
    "}\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "cat_cols = ['SEX', 'EDUCATION', 'MARRIAGE', 'age_group']\n",
    "numeric_cols = df.columns.drop(cat_cols + [\"Y\"]).tolist()\n",
    "\n",
    "# Create a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_cols)])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('Y', axis=1), df['Y'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to integer\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    X_train_temp = X_train.copy()\n",
    "    y_train_temp = y_train.copy()\n",
    "    X_test_temp = X_test.copy()\n",
    "    y_test_temp = y_test.copy()\n",
    "\n",
    "    # Create preprocessing and training pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               (model_name, model)])\n",
    "\n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train_temp, y_train_temp)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test_temp)\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f'Accuracy of {model_name}: {accuracy_score(y_test_temp, y_pred)}')\n",
    "\n",
    "    # Print confusion matrix\n",
    "    print(f'Confusion Matrix of {model_name}:\\n {confusion_matrix(y_test_temp, y_pred)}')\n",
    "\n",
    "    # Print classification report\n",
    "    print(f'Classification Report of {model_name}:\\n {classification_report(y_test_temp, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec708e",
   "metadata": {},
   "source": [
    "## What techniques are being used for modeling? (Justifications on Model Selection from High to Low Bias)\n",
    "\n",
    "1. Logistic Regression: Logistic Regression is our starting point as it's a simple yet effective algorithm for binary classification problems like ours. Despite its simplicity, Logistic Regression can perform well when features have a linear relationship with the log-odds of the outcome (default or not default). Moreover, logistic regression models are very interpretable, which is a bonus when we are first trying to understand our data.\n",
    "\n",
    "2. Support Vector Machines: We move to SVMs when we suspect that the boundaries between default and non-default credit card users might not be linear. SVMs can use kernel functions to handle such non-linearity. Also, SVMs can handle high-dimensional data well, which is relevant given the number of features we might have in a credit card dataset.\n",
    "\n",
    "3. Decision Trees: Decision Trees are chosen for their ability to handle non-linear relationships and their interpretability. They are more flexible than both Logistic Regression and SVMs and do not require any assumptions about the relationship between features and the target variable. Moreover, they can handle both numerical and categorical variables which are common in credit card data.\n",
    "\n",
    "4. AdaBoost: AdaBoost, an ensemble method, is used to potentially improve the performance of our Decision Tree. By combining multiple weak learners (small decision trees), AdaBoost forms a more robust model that can generalize better to unseen data. It is especially useful if some of our features are weakly associated with the outcome but collectively they can predict the outcome well.\n",
    "\n",
    "5. Random Forest: Random Forest is another ensemble model that creates a bunch of decision trees and aggregates their predictions. It's less likely to overfit than a single decision tree, which makes it a good choice if we have a lot of features and are worried about overfitting. Given the high-dimensional nature of credit card data, Random Forest is likely to improve our predictions.\n",
    "\n",
    "6. EasyEnsemble: We finally move to EasyEnsemble when tackling the class imbalance in our dataset. Given that defaults are typically less frequent than non-defaults in credit card datasets, EasyEnsemble helps by creating balanced subsets of data and using an ensemble of classifiers (each trained on a different subset). This approach ensures that our model is exposed to enough default examples during training and hence can generalize better to default cases in unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d5d52",
   "metadata": {},
   "source": [
    "# PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Standardize the numeric features\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "                                      ('pca', PCA(n_components=0.95))])\n",
    "\n",
    "# One hot encode the categorical features\n",
    "categorical_transformer = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Create the preprocessor using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)])\n",
    "\n",
    "# Fit and transform the train data, then transform the test data\n",
    "X_train_pca = preprocessor.fit_transform(X_train)\n",
    "X_test_pca = preprocessor.transform(X_test)\n",
    "\n",
    "# For this setup, you're not directly accessing the PCA object, so if you want the number of components and explained variance,\n",
    "# you'll have to extract it from the transformer:\n",
    "pca_components = preprocessor.named_transformers_['num']['pca'].n_components_\n",
    "explained_variance = preprocessor.named_transformers_['num']['pca'].explained_variance_ratio_\n",
    "\n",
    "print(\"Number of PCA components: \", pca_components)\n",
    "print(\"Explained variance: \", explained_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce52ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train_pca, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f'Accuracy of {model_name}: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "    # Print confusion matrix\n",
    "    print(f'Confusion Matrix of {model_name}:\\n {confusion_matrix(y_test, y_pred)}')\n",
    "\n",
    "    # Print classification report\n",
    "    print(f'Classification Report of {model_name}:\\n {classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bce6694",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Specify parameter grid for each model\n",
    "param_grid = {\n",
    "    'Logistic Regression': {\n",
    "        'Logistic Regression__C': [0.1, 1.0, 10.0, 100.0]\n",
    "    },\n",
    "    'Support Vector Machines': {\n",
    "        'Support Vector Machines__C': [0.1, 1.0, 10.0, 100.0]\n",
    "    },\n",
    "    'Decision Trees': {\n",
    "        'Decision Trees__max_depth': [50, 100, 500, 1000],\n",
    "        'Decision Trees__min_samples_split': [2, 5, 10],\n",
    "        'Decision Trees__min_samples_leaf': [1, 2, 5]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'Random Forest__n_estimators': [50, 100, 200, 500],\n",
    "        'Random Forest__max_depth': [50, 100, 500],\n",
    "        'Random Forest__min_samples_split': [2, 5, 10, 20],\n",
    "        'Random Forest__min_samples_leaf': [1, 2, 5, 10]\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'AdaBoost__n_estimators': [10, 50, 100, 200, 500, 1000],\n",
    "        'AdaBoost__learning_rate': [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "    },\n",
    "    'EasyEnsemble': {\n",
    "    'EasyEnsemble__n_estimators': [10, 50, 100, 200],\n",
    "    'EasyEnsemble__sampling_strategy': ['auto', 0.5, 0.7]\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "# Define a function to execute grid search on a model\n",
    "def perform_grid_search(model_name, model):\n",
    "    # Copy the train datasets\n",
    "    X_train_temp = X_train_pca.copy()\n",
    "    y_train_temp = y_train.copy()\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               (model_name, model)])\n",
    "\n",
    "    # Define the GridSearchCV object\n",
    "    grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='recall')\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    grid_search.fit(X_train_temp, y_train_temp)\n",
    "\n",
    "    # Print the best parameters and the corresponding score\n",
    "    print(f'Model: {model_name}')\n",
    "    print(f'Best parameters: {grid_search.best_params_}')\n",
    "    print(f'Best recall: {grid_search.best_score_}\\n')\n",
    "\n",
    "# Execute grid search on each model\n",
    "for model_name, model in models.items():\n",
    "    perform_grid_search(model_name, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa59d2",
   "metadata": {},
   "source": [
    "## Complexity of the dataset:\n",
    "\n",
    "1. **Did we use raw features or apply feature engineering?** \n",
    "We applied feature engineering. Using PCA, we changed the original data into a set of new features that capture most of the data's variation. This helps when dealing with big datasets by reducing unnecessary details and potentially improving our model's predictions.\n",
    "\n",
    "2. **How did we manage the dataset's size and complexity?** \n",
    "We used PCA to handle the dataset's complexity. This method finds and uses the main patterns in the data, reducing the number of features. The number of these main patterns or \"principal components\" we kept changes the dataset's size.\n",
    "On top of that, we tweaked settings in our machine learning algorithms, a process called hyperparameter tuning. While this doesn't directly reduce data size, it helps our model work better with the transformed data.\n",
    "By using PCA and tuning hyperparameters, we aim to make our model predict better. Still, it's essential for us to keep checking how well our model works, especially on new data it hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d7e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b368492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
